{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "kexQrXU-DjzY",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajat-Yd/Internship_First_Project.repo/blob/main/ML_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Glassdoor Job, Regression problem ML model\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project focuses on using the Machine Learning (ML) and Natural Language Processing (NLP) to analyze employee reviews from Glassdoor.\n",
        "\n",
        "Our objective is to develop an intelligent system capable of processing unstructured textual data to predict company ratings, classify sentiments, and identify key themes within employee feedback.\n",
        "\n",
        "Utilizing NLP and data science methodologies, the project aims to provide actionable insights for organizations, HR professionals, and job seekers to facilitate data-driven decision-making.\n",
        "\n",
        "The developmnt pipeline follows a systematic approach, which covers the data acquisition, preprocessing, exploratory data analysis (EDA), model development, evaluation, and final deployment. I will use this colab notebook to showcase this project."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Glassdoor gives us a vast collection of employee reviews which offer critical insights into workplace environments, managerial effectiveness, and job satisfaction. still, manually parsing thousands of reviews to identify trends, detect concerns, and predict company ratings is highly inefficient and impractical.\n",
        "\n",
        "**This project proposes an AI-driven system designed to:**\n",
        "\n",
        "- Conduct sentiment analysis on textual reviews.\n",
        "\n",
        "- Predict company ratings based on review content.\n",
        "\n",
        "- Extract and categorize key themes from employee feedback.\n",
        "\n",
        "- Detect biases in ratings influenced by factors such as job role, department, and location."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Natural Language Processing (NLP)\n",
        "import re  # Regular expressions for text processing\n",
        "import nltk  # NLP toolkit\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H00wCFXLylyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Rajat_AI ML_Project/glassdoor_jobs.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(\"Dataset Loaded Successfully!\\n\")\n",
        "print(\"First 5 Rows:\\n\", df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_columns = df.shape\n",
        "\n",
        "print(f\"Number of Rows: {num_rows}\")\n",
        "print(f\"Number of Columns: {num_columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"\\nDataset Info:\\n\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of Duplicate Rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing Values per Column:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values (sometimes different from NaN)\n",
        "null_values = df.isna().sum()\n",
        "print(\"\\nNull Values per Column:\\n\", null_values)# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Summary (Glassdoor Jobs Data)**  \n",
        "\n",
        "**Basic Information**\n",
        "- **Total Rows:** 956 (Job postings)  \n",
        "- **Total Columns:** 15 (Job-related attributes)  \n",
        "- **Dataset Type:** Structured CSV  \n",
        "\n",
        "**Data Quality Analysis**\n",
        "-  **No missing values** detected at a basic level.  \n",
        "-  **Salary Estimate & Revenue** need cleaning (parsing text to numerical values).  \n",
        "-  **Competitors Column** has `-1` values, meaning missing data.  \n",
        "-  **No duplicate rows** (if found, they should be removed).  "
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Dataset Columns:\\n\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Summary statistics of numerical columns\n",
        "print(\"Numerical Column Summary:\\n\")\n",
        "print(df.describe())\n",
        "\n",
        "# Summary statistics of categorical columns\n",
        "print(\"\\nCategorical Column Summary:\\n\")\n",
        "print(df.describe(include=\"object\"))\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variable Description**  \n",
        "\n",
        "- **Job Title** → Role of the job (e.g., \"Data Scientist\").  \n",
        "- **Job Description** → Detailed job responsibilities (useful for NLP).  \n",
        "- **Rating** → Glassdoor company rating (1-5 scale).  \n",
        "- **Company Name** → Name of the employer.  \n",
        "- **Location** → Job location (City, State format).  \n",
        "- **Headquarters** → Company's headquarters location.  \n",
        "- **Size** → Number of employees (e.g., \"1001-5000 employees\").  \n",
        "- **Founded** → Year company was established.  \n",
        "- **Type of Ownership** → Public, Private, Government, etc.  \n",
        "- **Industry** → Specific industry sector (e.g., \"Tech\", \"Healthcare\").  \n",
        "- **Sector** → Broader industry category.  \n",
        "- **Revenue** → Revenue range (text format, needs parsing).  \n",
        "- **Competitors** → List of competing companies (may have missing values).  \n",
        "- **Min Salary** → Extracted minimum salary (numeric).  \n",
        "- **Max Salary** → Extracted maximum salary (numeric).  \n",
        "- **Avg Salary** → Average of Min & Max Salary.  \n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Unique Values Count per Column:\\n\")\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'Salary Estimate' exists\n",
        "if 'Salary Estimate' in df.columns:\n",
        "    # Remove rows where Salary Estimate is '-1' (invalid salary values)\n",
        "    df = df[df['Salary Estimate'] != '-1']\n",
        "\n",
        "    # Remove text in parentheses (e.g., \"Glassdoor est.\")\n",
        "    df['Salary Estimate'] = df['Salary Estimate'].apply(lambda x: x.split('(')[0].strip())\n",
        "\n",
        "    # Handle special cases where salary is \"Employer Provided Salary: X-Y\"\n",
        "    df['Salary Estimate'] = df['Salary Estimate'].apply(lambda x: x.replace(\"Employer Provided Salary:\", \"\").strip())\n",
        "\n",
        "    # Identify hourly salary rows (contain \"Per Hour\") and yearly salary rows\n",
        "    df['Hourly'] = df['Salary Estimate'].apply(lambda x: 1 if 'Per Hour' in x else 0)\n",
        "\n",
        "    # Remove \"Per Hour\" and \"Per Year\" text\n",
        "    df['Salary Estimate'] = df['Salary Estimate'].apply(lambda x: x.replace('Per Hour', '').replace('Per Year', '').strip())\n",
        "\n",
        "    # Extract Min & Max Salary safely\n",
        "    df['Min Salary'] = df['Salary Estimate'].apply(lambda x: int(x.split('-')[0].replace('K', '').replace('$', '').strip()) * 1000)\n",
        "    df['Max Salary'] = df['Salary Estimate'].apply(lambda x: int(x.split('-')[1].replace('K', '').replace('$', '').strip()) * 1000)\n",
        "\n",
        "    # Convert Hourly Salary to Annual Salary (Assuming 40 hours/week, 52 weeks/year → 2080 hours)\n",
        "    df.loc[df['Hourly'] == 1, 'Min Salary'] = df['Min Salary'] * 2080\n",
        "    df.loc[df['Hourly'] == 1, 'Max Salary'] = df['Max Salary'] * 2080\n",
        "\n",
        "    # Calculate Average Salary\n",
        "    df['Avg Salary'] = (df['Min Salary'] + df['Max Salary']) / 2\n",
        "\n",
        "    # Drop the original Salary Estimate column\n",
        "    df.drop(['Salary Estimate', 'Hourly'], axis=1, inplace=True)\n",
        "\n",
        "    print(\"✅ Salary column cleaned successfully!\")\n",
        "else:\n",
        "    print(\"⚠️ Warning: 'Salary Estimate' column not found. Check column names above.\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uYS3pIoo7gne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Removed Duplicate Rows**\n",
        "- **Before:** The dataset may have had duplicate job postings.  \n",
        "- **After:** Removed duplicates to ensure unique job listings.  \n",
        "- **Impact:** Avoids redundancy and improves model performance.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Handled Missing Values**\n",
        "- **Fixed Competitors Column:**  \n",
        "  - Replaced `\"-1\"` with `\"No Competitors\"` (indicating missing data).  \n",
        "- **Dropped rows with missing values in critical columns:**  \n",
        "  - Ensured **Job Title & Job Description** are always present.\n",
        "---\n",
        "\n",
        "### **3. Split `\"Location\"` into `\"City\"` & `\"State\"`**\n",
        "- **Before:** `\"Location\"` was a single column with `\"City, State\"` format.  \n",
        "- **After:**  \n",
        "  - Split `\"Location\"` into separate `\"City\"` & `\"State\"` columns.  \n",
        "  - Filled missing values with `\"Unknown\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Converted `\"Founded\"` Year into `\"Company Age\"`**\n",
        "- **Before:** `\"Founded\"` column contained raw years (e.g., `1990`).  \n",
        "- **After:**  \n",
        "  - Converted `\"Founded\"` into `\"Company Age\"` (`2025 - Founded`).  \n",
        "  - Replaced `\"-1\"` (unknown values) with `NaN`.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Key Insights from the Dataset**  \n",
        "\n",
        "### **1. Company Ratings & Job Satisfaction**\n",
        "- Glassdoor ratings range from **1.0 to 5.0**.    \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Salary Trends**\n",
        "- **Hourly Wages** were converted to yearly salaries (assuming **2080 working hours/year**).  \n",
        "- **Min, Max, and Avg Salaries were extracted** from the salary column.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Job Location Analysis**\n",
        "- The dataset has jobs from **multiple states in the U.S.**  \n",
        "- Certain **states/cities dominate hiring.**\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Industry & Sector Breakdown**\n",
        "- The dataset contains jobs from **Tech, Healthcare, Finance, and Consulting** sectors.  \n",
        "- Some industries have **more competitive salaries and ratings** than others.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1\n",
        "Salary distribution"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better visuals\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# salary distribution/\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Avg Salary'], bins=30, kde=True, color='blue')\n",
        "plt.title(\"Distribution of Average Salaries\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary (USD)\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a histogram with a KDE (Kernel Density Estimation) curve because it is ideal for visualizing distributions.\n",
        "\n",
        "The KDE curve provides a smooth representation of salary density, making trends easier to analyze."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most job salaries fall within a specific range, likely between $50K-$150K.\n",
        "\n",
        "There might be outliers with extremely high salaries, which could indicate executive roles or highly specialized positions.\n",
        "\n",
        "If there are multiple peaks, it indicates the presence of different salary groups based on seniority (entry-level vs. senior roles) or industries."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes!\n",
        "For companies:\n",
        "\n",
        "- Understanding salary distribution helps in structuring competitive salary packages to attract top talent.\n",
        "Identifies salary gaps, ensuring fair pay across different job levels.\n",
        "\n",
        "For job seekers:\n",
        "- Helps candidates negotiate salaries better, ensuring they align with market trends.\n",
        "Guides professionals on whether they should upskill or switch industries for better pay."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2\n",
        "---\n",
        "Salary v/s Company Ratings"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=df['Rating'], y=df['Avg Salary'], palette=\"coolwarm\")\n",
        "plt.title(\"Company Rating vs. Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Glassdoor Rating\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (USD)\", fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A boxplot is best for showing salary distribution across company ratings, highlighting median salaries, variations, and outliers."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Higher-rated companies (4.0+) tend to offer better salaries.\n",
        "----\n",
        "\n",
        "- Low-rated companies (below 3.0) show wider salary variation, indicating inconsistent pay structures."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "- Companies with higher salaries & ratings attract top talent.\n",
        "- Helps businesses balance salary & work culture improvements.\n",
        "\n",
        "Possible Negative Impact:\n",
        "- Some high-rated companies may offer lower salaries, relying on brand reputation instead of pay—leading to attrition."
      ],
      "metadata": {
        "id": "Z6PjGjTeFsg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3\n",
        "Top Hiring Cities"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "top_cities = df['Location'].value_counts().head(10)\n",
        "sns.barplot(x=top_cities.index, y=top_cities.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 10 Cities with Most Job Postings\", fontsize=14)\n",
        "plt.xlabel(\"City\", fontsize=12)\n",
        "plt.ylabel(\"Number of Job Postings\", fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart effectively shows which cities have the highest job postings, helping identify key hiring locations."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Major tech hubs (e.g., San Francisco, New York) dominate hiring.\n",
        "\n",
        "- Some unexpected cities may have high job demand, indicating emerging job markets."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "- Helps companies focus hiring efforts in high-demand cities.\n",
        "- Job seekers can target cities with better employment opportunities.\n",
        "---\n",
        "Possible Negative Impact:\n",
        "\n",
        "- High job concentration in a few cities may cause talent shortages elsewhere."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4\n",
        "Industry vs salary"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "top_industries = df.groupby(\"Industry\")[\"Avg Salary\"].mean().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=top_industries.index, y=top_industries.values, palette=\"magma\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top Paying Industries (Average Salary)\", fontsize=14)\n",
        "plt.xlabel(\"Industry\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (USD)\", fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for comparing average salaries across industries, making it easy to spot high-paying vs. low-paying sectors.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tech, Finance, and Healthcare industries tend to offer the highest salaries."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "- Helps companies benchmark salaries to stay competitive in their industry.\n",
        "\n",
        "\n",
        "Possible Negative Impact:\n",
        "- Salary gaps across industries may cause talent migration from low-paying fields, leading to shortages in critical sectors like education & social work.\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5\n",
        "Company age vs salary"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=df['Founded'], y=df['Avg Salary'], alpha=0.6, color='green')\n",
        "plt.title(\"Company Age vs. Average Salary\", fontsize=14)\n",
        "plt.xlabel(\"Company Age (Years)\", fontsize=12)\n",
        "plt.ylabel(\"Average Salary (USD)\", fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is ideal for visualizing the relationship between company age and salary, showing patterns across different experience levels."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Younger startups (0-10 years) may offer higher salaries to attract talent.\n",
        "- Older companies (50+ years) tend to have stable but moderate salaries.\n",
        "- Mid-aged companies (10-50 years) show a balanced salary structure."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "- Helps job seekers choose between startups vs. established firms based on salary potential.\n",
        "\n",
        "Possible Negative Impact:\n",
        "- Younger companies may struggle with retention if high salaries aren’t sustainable."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select only numerical columns\n",
        "num_df = df.select_dtypes(include=['int64', 'float64'])  # Keep only numeric data\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = num_df.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# Title\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\", fontsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is the best way to visualize correlations between numerical variables, helping identify strong positive or negative relationships."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Salary vs. Rating - If positively correlated, higher-rated companies tend to pay better.\n",
        "\n",
        "- Salary vs. Company Age - If negatively correlated, younger companies may offer higher salaries to attract talent.\n",
        "\n",
        "- Company Age vs. Rating - If positively correlated, older companies tend to have better reputations."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numerical columns for pair plot\n",
        "num_df = df[['Avg Salary', 'Min Salary', 'Max Salary', 'Rating', 'Founded']]\n",
        "\n",
        "# Create pair plot\n",
        "sns.pairplot(num_df, diag_kind='kde', plot_kws={'alpha':0.6})\n",
        "\n",
        "# Show the plot\n",
        "plt.suptitle(\"Pair Plot of Key Numerical Features\", fontsize=14, y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot helps visualize relationships between multiple numerical variables, showing scatter plots for comparisons and histograms for distributions."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Salary vs. Rating → If positively correlated, higher-rated companies pay more.\n",
        "\n",
        "- Min Salary vs. Max Salary → Should show a strong positive correlation since they are linked.\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.drop(['Competitors'], axis=1, inplace=True)  # Drop columns with excessive missing values\n",
        "print(\"✅ Columns with high missing values removed!\")"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropped columns with excessive missing values (e.g., Competitors) → If too many values are missing, they don’t contribute to analysis.\n",
        "\n",
        "- Filled numerical values with the median → Median is less affected by outliers than the mean, making it more reliable for skewed salary/rating data.\n",
        "\n",
        "- Filled categorical values with the mode → Mode (most frequent value) is best for categorical data, ensuring consistency without distorting distributions."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting outlier's"
      ],
      "metadata": {
        "id": "EoAOwihH8pGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing outliers in numerical columns\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df[['Min Salary', 'Max Salary', 'Avg Salary', 'Rating', 'Founded']], palette=\"Set2\")\n",
        "plt.title(\"Boxplot for Outlier Detection\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FuHo85Ru8oVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "# Apply to numerical columns\n",
        "num_cols = ['Min Salary', 'Max Salary', 'Avg Salary', 'Rating', 'Founded']\n",
        "for col in num_cols:\n",
        "    df = remove_outliers(df, col)\n",
        "\n",
        "print(\"✅ Outliers removed using IQR method!\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the removal of Outlier's"
      ],
      "metadata": {
        "id": "UIgRmJpd81oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check boxplot again after removing outliers\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df[num_cols], palette=\"Set2\")\n",
        "plt.title(\"Boxplot After Outlier Removal\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sK4_Ctwi84-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boxplot Visualization → To visually detect extreme values in salary, rating, and company age.\n",
        "\n",
        "- IQR (Interquartile Range) Method → Removed outliers beyond 1.5x IQR since it's effective for skewed salary distributions without removing too much data."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Identify categorical columns\n",
        "categorical_cols = ['Job Title', 'Company Name', 'Industry', 'Sector', 'Type of ownership', 'Location', 'Job Description', 'Headquarters', 'Size', 'Revenue']\n",
        "\n",
        "# Apply Label Encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Save encoders for future reference\n",
        "\n",
        "print(\"✅ Categorical features encoded successfully!\")"
      ],
      "metadata": {
        "id": "8PNN075qMxol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Fill missing categorical values with the most frequent value (mode)\n",
        "cat_cols = ['Job Title', 'Company Name', 'Industry', 'Sector', 'Location']\n",
        "for col in cat_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"✅ Missing categorical values filled with mode!\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recheck for missing values\n",
        "print(\"Remaining Missing Values After Handling:\\n\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "fx3ApRQz9ev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the missing values are filled ? or not"
      ],
      "metadata": {
        "id": "90AVVm1s9Wqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Label Encoding → Used for categorical features like \"Job Title\", \"Company Name\", and \"Industry\" because ML models need numerical values.\n",
        "\n",
        "- Mode Imputation for Missing Categorical Values → Ensures that categorical data remains consistent without introducing bias."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "-nbpnPPt-uSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "# Function to expand contractions in text columns\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text) if isinstance(text, str) else text\n",
        "\n",
        "# Apply to text-based columns\n",
        "text_cols = ['Job Description']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].apply(expand_contractions)\n",
        "\n",
        "print(\"✅ Contractions expanded successfully!\")\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['Job Title'] = df['Job Title'].str.lower()\n",
        "df['Company Name'] = df['Company Name'].str.lower()\n",
        "df['Industry'] = df['Industry'].str.lower()\n",
        "df['Sector'] = df['Sector'].str.lower()\n",
        "df['Location'] = df['Location'].str.lower()\n",
        "df['Job Description'] = df['Job Description'].str.lower()\n",
        "\n",
        "print(\"✅ Text converted to lowercase!\")"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation)) if isinstance(text, str) else text\n",
        "\n",
        "# Apply to text-based columns\n",
        "text_cols = ['Job Description']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].apply(remove_punctuation)\n",
        "\n",
        "print(\"✅ Punctuation removed successfully!\")"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+', '', text) if isinstance(text, str) else text\n",
        "\n",
        "# Function to remove digits\n",
        "def remove_digits(text):\n",
        "    return re.sub(r'\\d+', '', text) if isinstance(text, str) else text\n",
        "\n",
        "# Apply to text-based columns\n",
        "text_cols = ['Job Description']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].apply(remove_urls).apply(remove_digits)\n",
        "\n",
        "print(\"✅ URLs and digits removed successfully!\")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Remove White spaces\n",
        "\n",
        "# Remove White spacesimport nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# # Download stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word.lower() not in stop_words]) if isinstance(text, str) else text\n",
        "\n",
        "# Function to remove extra whitespaces\n",
        "def remove_whitespace(text):\n",
        "    return \" \".join(text.split()) if isinstance(text, str) else text\n",
        "\n",
        "# Apply to text-based columns\n",
        "text_cols = ['Job Description']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].apply(remove_stopwords).apply(remove_whitespace)\n",
        "\n",
        "print(\"✅ Stopwords and extra whitespaces removed successfully!\")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function to rephrase text using TextBlob\n",
        "def rephrase_text(text):\n",
        "    if isinstance(text, str):\n",
        "        blob = TextBlob(text)\n",
        "    else:\n",
        "        return str(blob.correct())  # Corrects spelling & rephrases slightly\n",
        "    return text\n",
        "\n",
        "# Apply to text-based columns\n",
        "text_cols = ['Job Description']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].apply(rephrase_text)\n",
        "\n",
        "print(\"✅ Text rephrased successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download tokenizer if not available\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Function to tokenize words\n",
        "def tokenize_words(text):\n",
        "    return word_tokenize(text) if isinstance(text, str) else text\n",
        "\n",
        "# Function to tokenize sentences\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text) if isinstance(text, str) else text\n",
        "\n",
        "# Apply word tokenization\n",
        "df['Job Description Tokens'] = df['Job Description'].apply(tokenize_words)\n",
        "\n",
        "# Apply sentence tokenization (optional)\n",
        "# df['Job Description Sentences'] = df['Job Description'].apply(tokenize_sentences)\n",
        "\n",
        "print(\"✅ Text tokenized successfully!\")\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def apply_lemmatization(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()]) if isinstance(text, str) else text\n",
        "\n",
        "# Apply lemmatization to job descriptions\n",
        "df['Job Description Lemmatized'] = df['Job Description'].apply(apply_lemmatization)\n",
        "\n",
        "print(\"✅ Lemmatization applied successfully!\")"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to apply stemming\n",
        "def apply_stemming(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()]) if isinstance(text, str) else text\n",
        "\n",
        "# Apply stemming to job descriptions\n",
        "df['Job Description Stemmed'] = df['Job Description'].apply(apply_stemming)\n",
        "\n",
        "print(\"✅ Stemming applied successfully!\")"
      ],
      "metadata": {
        "id": "7KIJKftF6vhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stemming is faster but less accurate, useful for reducing word variations.\n",
        "- Lemmatization is slower but more precise, useful for meaning-based NLP tasks.\n",
        "- Both techniques improve text consistency for feature extraction & machine learning models."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary datasets\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Optional, improves lemmatization accuracy\n",
        "\n",
        "print(\"✅ NLTK resources downloaded successfully!\")"
      ],
      "metadata": {
        "id": "tweX4KbS8Bge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to apply POS tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return pos_tag(tokens) if isinstance(text, str) else text\n",
        "\n",
        "# Apply POS tagging to job descriptions\n",
        "df['Job Description POS'] = df['Job Description'].apply(pos_tagging)\n",
        "\n",
        "print(\"POS tagging applied sucessfully\")"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize text for Word2Vec\n",
        "df['Job Description Tokenized'] = df['Job Description'].apply(word_tokenize)\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=df['Job Description Tokenized'], vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "print(\"✅ Word2Vec model trained successfully!\")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Word2Vec → Captures semantic meaning & word relationships for deep NLP models."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Create a new feature: Salary Range\n",
        "df['Salary Range'] = df['Max Salary'] - df['Min Salary']\n",
        "\n",
        "print(\"✅ New feature 'Salary Range' created successfully!\")\n",
        "\n",
        "# Define a threshold (e.g., $100,000) to classify high-paying jobs\n",
        "df['High Paying'] = df['Avg Salary'].apply(lambda x: 1 if x >= 100000 else 0)\n",
        "\n",
        "print(\"✅ New feature 'High Paying' (1 = Yes, 0 = No) created successfully!\")"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Convert Categorical Columns Using TF-IDF\n",
        "\n"
      ],
      "metadata": {
        "id": "qlbuCYFZLMBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=500)  # Keep top 500 words\n",
        "\n",
        "# Transform Job Descriptions into TF-IDF vectors\n",
        "tfidf_matrix = tfidf.fit_transform(df['Job Description'])\n",
        "\n",
        "# Convert to DataFrame and merge with the original dataset\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "df = df.reset_index(drop=True)  # Reset index for merging\n",
        "df = pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "# Identify non-numeric columns\n",
        "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Remove text-based columns from feature selection\n",
        "X = df.drop(columns=['Avg Salary'] + list(non_numeric_cols))  # Keep only numerical features\n",
        "y = df['Avg Salary']\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ Removed non-numeric columns for feature selection!\")\n",
        "print(\"✅ Text converted to numerical features using TF-IDF!\")"
      ],
      "metadata": {
        "id": "AxuzlT2uLPnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# Compute feature importance using mutual information\n",
        "feature_importance = mutual_info_regression(X, y)\n",
        "important_features = pd.Series(feature_importance, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "print(\"✅ Feature importance calculated successfully!\")\n",
        "print(important_features)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Variance Thresholding → Removes low-variance features that don’t contribute much to model learning.\n",
        "- Mutual Information Regression → Identifies how much each feature influences salary (target variable), ensuring only informative predictors are used."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Min Salary & Max Salary → Strong predictors for Avg Salary, as they directly define salary range.\n",
        "- Company Rating → Higher-rated companies tend to offer better salaries.\n",
        "- Company Age → Younger companies may offer competitive salaries to attract talent.\n",
        "- Industry & Sector → Some industries (e.g., tech, finance) offer higher average salaries.\n",
        "- Location (State) → Salaries differ significantly across states due to cost of living & job demand."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import numpy as np\n",
        "\n",
        "# Apply log transformation to skewed numerical features\n",
        "skewed_cols = ['Min Salary', 'Max Salary', 'Avg Salary', 'Founded']\n",
        "\n",
        "for col in skewed_cols:\n",
        "    df[col] = np.log1p(df[col])  # log1p(x) = log(1 + x) to avoid log(0) issues\n",
        "\n",
        "print(\"✅ Log transformation applied to skewed features!\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numerical columns for scaling\n",
        "num_cols = ['Min Salary', 'Max Salary', 'Avg Salary', 'Rating', 'Founded']\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Apply Min-Max Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df[num_cols] = minmax_scaler.fit_transform(df[num_cols])\n",
        "\n",
        "print(\"✅ Standardization applied to numerical features!\")\n",
        "print(\"✅ Min-Max Scaling applied to numerical features!\")\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Log Transformation → Reduces skewness & outliers for better model stability.\n",
        "- Standardization (Z-score) → Works best for models like SVM, Logistic Regression.\n",
        "- Min-Max Scaling → Works well for Deep Learning models & Distance-based ML models (KNN, Clustering)."
      ],
      "metadata": {
        "id": "Qwpax3gkEcw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ✔️ Removes redundant features to reduce complexity.\n",
        "- ✔️ Speeds up training & improves generalization."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No dimensionality is done."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df.drop(columns=['Avg Salary'])  # Independent variables\n",
        "y = df['Avg Salary']  # Target variable\n",
        "\n",
        "# Split data into 80% train & 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"✅ Data split successfully: Train Size: {X_train.shape[0]}, Test Size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for the taken ratio**\n",
        "- ✔️ 80% Training Data → Provides enough data for the model to learn patterns properly.\n",
        "- ✔️ 20% Testing Data → Ensures a good sample for evaluating performance without overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's check if the data is imbalanced or not**"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution of salaries\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Avg Salary'], bins=30, kde=True, color='blue')\n",
        "plt.title(\"Distribution of Average Salaries\", fontsize=14)\n",
        "plt.xlabel(\"Average Salary\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Check salary percentiles\n",
        "print(df['Avg Salary'].describe())"
      ],
      "metadata": {
        "id": "ZM2qSu1fGvbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# No immediate need of Hanalding Data imbalance right now."
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the given stats, the data does NOT appear to be highly imbalanced.\n",
        "- No immediate need for SMOTE or oversampling.\n",
        "- However, if the histogram shows extreme skewness, I can apply log transformation or binning to improve distribution."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing list based coloumns with Data type =  Object**"
      ],
      "metadata": {
        "id": "HH5_iWHFbfrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_cols = ['Job Description POS', 'Job Description Tokens','Job Description Lemmatized', 'Job Description Stemmed']  # problematic columns\n",
        "X = X.drop(columns=[col for col in list_cols if col in X.columns])\n",
        "\n",
        "print(\"✅ Removed list-based columns for model training!\")\n",
        "\n",
        "print(\"✅ Data Types After Fix:\")\n",
        "print(X.dtypes.value_counts())  # Ensure only numerical features remain"
      ],
      "metadata": {
        "id": "vnvpLdMjSwIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear Regression Model\n",
        "model_1 = LinearRegression()\n",
        "model_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Model 1 trained successfully!\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a bar chart for evaluation metrics\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R² Score']\n",
        "values = [mae, mse, rmse, r2]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=metrics, y=values, palette='coolwarm')\n",
        "\n",
        "# Annotate bars with actual values\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=12)\n",
        "\n",
        "plt.title(\"📊 Model 1 Evaluation Metric Scores\", fontsize=14)\n",
        "plt.xlabel(\"Evaluation Metrics\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "20vKWuWeVVkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the Model Overfitting\n",
        "\n",
        "train_r2 = model_1.score(X_train, y_train)\n",
        "test_r2 = model_1.score(X_test, y_test)\n",
        "\n",
        "print(f\"📊 R² Score on Training Data: {train_r2:.4f}\")\n",
        "print(f\"📊 R² Score on Test Data: {test_r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "58l4aLs-WCAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred_1 = model_1.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred_1)\n",
        "mse = mean_squared_error(y_test, y_pred_1)\n",
        "rmse = mean_squared_error(y_test, y_pred_1)\n",
        "r2 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"📊 Model 1 - Linear Regression Performance:\")\n",
        "print(f\"✅ Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"✅ Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"✅ Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"✅ R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "#IMport Required libraries\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Initialize Ridge Regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Apply GridSearchCV to find the best alpha\n",
        "grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameter\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "print(f\"✅ Best Alpha Found: {best_alpha}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Train the optimized Ridge model\n",
        "optimized_model_1 = Ridge(alpha=best_alpha)\n",
        "optimized_model_1.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Model 1 (Ridge Regression with Optimized Alpha) trained successfully!\")\n",
        "\n",
        "# Predict on the model\n",
        "# Make predictions\n",
        "y_pred_optimized = optimized_model_1.predict(X_test)\n",
        "\n",
        "print(\"✅ Predictions made successfully!\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}